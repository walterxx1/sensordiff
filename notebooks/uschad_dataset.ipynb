{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental mean: 3.0\n",
      "Incremental standard deviation: 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "class IncrementalStats:\n",
    "    def __init__(self):\n",
    "        # Initialize counters for the number of samples, mean, and variance\n",
    "        self.N = 0  # Total number of samples processed\n",
    "        self.mean = 0  # Current mean\n",
    "        self.variance = 0  # Current variance\n",
    "    \n",
    "    def update(self, new_data):\n",
    "        # Calculate batch size, mean, and variance of the new data batch\n",
    "        new_N = len(new_data)\n",
    "        new_mean = np.mean(new_data)\n",
    "        new_variance = np.var(new_data)\n",
    "        \n",
    "        # Update the total number of samples\n",
    "        total_N = self.N + new_N\n",
    "        \n",
    "        # Update the mean incrementally\n",
    "        updated_mean = (self.N * self.mean + new_N * new_mean) / total_N\n",
    "        \n",
    "        # Update the variance incrementally\n",
    "        if self.N > 0:\n",
    "            updated_variance = (\n",
    "                (self.N * self.variance + new_N * new_variance) / total_N\n",
    "                + (self.N * new_N * (self.mean - new_mean) ** 2) / (total_N ** 2)\n",
    "            )\n",
    "        else:\n",
    "            updated_variance = new_variance  # For the first batch\n",
    "        \n",
    "        # Update the internal state\n",
    "        self.N = total_N\n",
    "        self.mean = updated_mean\n",
    "        self.variance = updated_variance\n",
    "    \n",
    "    def get_mean(self):\n",
    "        return self.mean\n",
    "    \n",
    "    def get_std(self):\n",
    "        return np.sqrt(self.variance)\n",
    "\n",
    "# Example usage\n",
    "incremental_stats = IncrementalStats()\n",
    "\n",
    "# Simulating batch updates\n",
    "batch_1 = np.array([1.0, 2.0, 3.0])\n",
    "incremental_stats.update(batch_1)\n",
    "\n",
    "batch_2 = np.array([4.0, 5.0])\n",
    "incremental_stats.update(batch_2)\n",
    "\n",
    "print(f\"Incremental mean: {incremental_stats.get_mean()}\")\n",
    "print(f\"Incremental standard deviation: {incremental_stats.get_std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different activities need different means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity 10: Mean shape: [-0.5196329   0.59401958 -0.22174934 -0.00406078  0.00223029  0.00841752], Std shape: [0.3715384  0.41490181 0.32319146 2.10898974 1.41954232 2.32720041]\n",
      "Activity 11: Mean shape: [ 0.9270141   0.34147598 -0.10944372 -0.08597624 -0.04979096  0.04334709], Std shape: [0.04349677 0.09340629 0.11440484 4.59843404 3.82722627 2.1982048 ]\n",
      "Activity 12: Mean shape: [ 0.92741534  0.34003236 -0.10705996 -0.04559116 -0.07663385  0.08755785], Std shape: [0.0461052  0.10065777 0.11683802 5.97485646 4.23458415 3.08973422]\n",
      "Activity 1: Mean shape: [ 0.97165233  0.01000013 -0.10431792  0.74628317 -1.0839197   0.34396076], Std shape: [ 0.35685347  0.26722338  0.26153283 51.88362265 28.15745122 42.62614971]\n",
      "Activity 2: Mean shape: [ 0.97072791  0.02765025 -0.08869843 24.64892371  1.93437172 -2.40588278], Std shape: [ 0.36652257  0.27274009  0.2746495  56.84935069 27.49659484 43.16565851]\n",
      "Activity 3: Mean shape: [  0.968976     0.04728519  -0.08474551 -24.93846565  -1.74471529\n",
      "  -0.35874692], Std shape: [ 0.36240674  0.26930086  0.28989089 55.88141632 28.96260911 42.13189415]\n",
      "Activity 4: Mean shape: [  0.94602985  -0.02398505  -0.10823043 -16.70836422  -1.14337875\n",
      "  -0.28512815], Std shape: [ 0.36194886  0.30073861  0.24564034 54.94280322 29.06322359 43.04095603]\n",
      "Activity 5: Mean shape: [ 9.67830076e-01  2.60764083e-02 -7.42368517e-04  1.80499880e+01\n",
      "  1.28635857e+00 -5.47593050e-01], Std shape: [ 0.42568552  0.30086743  0.26627495 56.49386523 25.39075062 39.89155306]\n",
      "Activity 6: Mean shape: [ 0.95218173 -0.09588609 -0.1340238  -1.37819546 -2.02252171 -0.4710199 ], Std shape: [  1.24801025   0.71507192   0.57432202 123.37661832  65.69147217\n",
      " 132.50048222]\n",
      "Activity 7: Mean shape: [ 0.95533395 -0.00364853 -0.13532109 -1.18653025 -1.79545199 -0.06142908], Std shape: [ 1.44671708  0.52445935  0.37822606 62.23152205 40.85845519 80.26509939]\n",
      "Activity 8: Mean shape: [ 0.82366803  0.47126836  0.18492444 -0.74180564 -1.81118068  0.12689775], Std shape: [0.09719388 0.18897383 0.18050096 2.40570578 2.66880713 3.47253378]\n",
      "Activity 9: Mean shape: [ 0.95086504  0.15918266 -0.13840083 -0.33086761 -0.83871418  0.02334645], Std shape: [ 0.03962262  0.19574575  0.10203384 11.18896754  3.96604519  4.81971583]\n",
      "check min and max on train [-2.60144896 -5.27302944 -3.77084032 -4.52264416 -3.614557   -5.00721404] [3.01721383 2.83070026 2.37800819 3.58713687 3.63187192 5.30933523]\n",
      "check min and max on test [-2.48774921 -2.71961521 -2.97540379 -2.60472825 -3.05552239 -2.16759233] [3.01721383 2.83070026 2.37800819 3.58713687 3.63187192 5.30933523]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "class IncrementalStats:\n",
    "    def __init__(self):\n",
    "        self.N = 0\n",
    "        self.mean = None\n",
    "        self.variance = None\n",
    "    \n",
    "    def update(self, new_data):\n",
    "        new_N = new_data.shape[0]\n",
    "        new_mean = np.mean(new_data, axis=0)\n",
    "        new_variance = np.var(new_data, axis=0)\n",
    "        \n",
    "        if self.mean is None:\n",
    "            self.mean = new_mean\n",
    "            self.variance = new_variance\n",
    "            self.N = new_N\n",
    "        else:\n",
    "            total_N = self.N + new_N\n",
    "            updated_mean = (self.N * self.mean + new_N * new_mean) / total_N\n",
    "            updated_variance = (\n",
    "                (self.N * self.variance + new_N * new_variance) / total_N\n",
    "                + (self.N * new_N * (self.mean - new_mean) ** 2) / (total_N ** 2)\n",
    "            )\n",
    "            self.N = total_N\n",
    "            self.mean = updated_mean\n",
    "            self.variance = updated_variance\n",
    "\n",
    "    def get_mean(self):\n",
    "        return self.mean\n",
    "    \n",
    "    def get_std(self):\n",
    "        return np.sqrt(self.variance)\n",
    "\n",
    "\n",
    "# Initialize the parameter dictionary\n",
    "activity_para_dict = {}\n",
    "\n",
    "# File paths and dataset\n",
    "uschad_data_path = './Datas/USC_HAD_dataset.h5'\n",
    "train_list = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Open the dataset\n",
    "with h5py.File(uschad_data_path, 'r') as f_r:\n",
    "    data_grp = f_r['datas']\n",
    "    \n",
    "    # Loop through the dataset keys\n",
    "    for key in data_grp:\n",
    "        # Extract subject and activity indices\n",
    "        subject_i = int(key.split('_')[0].split('b')[1])\n",
    "        activity_i = key.split('_')[1].split('a')[1]\n",
    "        \n",
    "        # Skip if the subject is not in the train list\n",
    "        if subject_i not in train_list:\n",
    "            continue\n",
    "\n",
    "        # Load the data for this trial\n",
    "        trial_data = np.array(data_grp[key])\n",
    "        \n",
    "        # Initialize IncrementalStats for each activity if not already in the dictionary\n",
    "        if activity_i not in activity_para_dict:\n",
    "            activity_para_dict[activity_i] = IncrementalStats()\n",
    "\n",
    "        # Update the stats for the current activity\n",
    "        activity_para_dict[activity_i].update(trial_data)\n",
    "\n",
    "# Save the mean and std of each activity to an HDF5 file\n",
    "output_h5py_path = './Datas/uschad_activity_parameters.h5'\n",
    "with h5py.File(output_h5py_path, 'w') as f_w:\n",
    "    for activity, stats in activity_para_dict.items():\n",
    "        mean = stats.get_mean()\n",
    "        std = stats.get_std()\n",
    "        \n",
    "        # Save the mean and std for this activity in the HDF5 file\n",
    "        f_w.create_dataset(f'{activity}/mean', data=mean)\n",
    "        f_w.create_dataset(f'{activity}/std', data=std)\n",
    "\n",
    "        # Print for checking\n",
    "        print(f'Activity {activity}: Mean shape: {mean}, Std shape: {std}')\n",
    "\n",
    "\n",
    "# Applying the calculated parameters during data processing\n",
    "def apply_normalization(data, activity, activity_para_dict):\n",
    "    \"\"\"Applies the mean and std normalization for the given activity.\"\"\"\n",
    "    mean = activity_para_dict[activity].get_mean()\n",
    "    std = activity_para_dict[activity].get_std()\n",
    "    \n",
    "    # Normalize the data\n",
    "    return (data - mean) / (std + 1e-8)  # Adding a small constant to avoid division by zero\n",
    "\n",
    "\n",
    "# Example usage: Load a test trial and normalize based on the activity mean/std\n",
    "with h5py.File(uschad_data_path, 'r') as f_r:\n",
    "    test_trial = f_r['datas']['sub12_a3_trial1']  # Example trial\n",
    "    activity = '3'  # Activity index\n",
    "    normalized_data_test = apply_normalization(test_trial, activity, activity_para_dict)\n",
    "    \n",
    "    train_trial = f_r['datas']['sub1_a3_trial1']\n",
    "    normalized_data_train = apply_normalization(train_trial, activity, activity_para_dict)\n",
    "    print('check min and max on train', np.min(normalized_data_train, 0), np.max(normalized_data_train, 0))\n",
    "    print('check min and max on test', np.min(normalized_data_test, 0), np.max(normalized_data_train, 0))\n",
    "    # print(f'Normalized data for sub12_a3_trial1: {normalized_data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate different activities, save original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check shape sleeping 375000\n",
      "check shape elevatorup 165491\n",
      "check shape elevatordown 164999\n",
      "check shape walkingforward 381400\n",
      "check shape walkingleft 258800\n",
      "check shape walkingright 275500\n",
      "check shape walkingupstairs 211800\n",
      "check shape walkingdownstairs 197400\n",
      "check shape runningforward 176500\n",
      "check shape jumping 107100\n",
      "check shape sitting 261500\n",
      "check shape standing 236000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "uschad_data_path = './Datas/uschad_dataset.h5'\n",
    "uschad_activity_path = './Datas/uschad_activity_dataset.h5'\n",
    "\n",
    "activity_name_dict = {\n",
    "    1: \"walkingforward\",\n",
    "    2: \"walkingleft\",\n",
    "    3: \"walkingright\",\n",
    "    4: \"walkingupstairs\",\n",
    "    5: \"walkingdownstairs\",\n",
    "    6: \"runningforward\",\n",
    "    7: \"jumping\",\n",
    "    8: \"sitting\",\n",
    "    9: \"standing\",\n",
    "    10: \"sleeping\",\n",
    "    11: \"elevatorup\",\n",
    "    12: \"elevatordown\"\n",
    "}\n",
    "\n",
    "def sliding_window(dataset, window_size, step_size):\n",
    "    data_len = dataset.shape[0]\n",
    "    num_windows = (data_len - window_size) // step_size + 1\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * step_size\n",
    "        windows.append(dataset[start_idx : start_idx + window_size, :])\n",
    "    return np.array(windows)\n",
    "\n",
    "activity_data_dict = {}\n",
    "scaler = StandardScaler()\n",
    "with h5py.File(uschad_data_path, 'r') as f_r:\n",
    "    data_grp = f_r['datas']\n",
    "    for key in data_grp:\n",
    "        \n",
    "        key_data = data_grp[key][:]\n",
    "        \n",
    "        activity_i = key.split('_')[1].split('a')[1]\n",
    "        activity_name = activity_name_dict[int(activity_i)]\n",
    "        if activity_name not in activity_data_dict:\n",
    "            activity_data_dict[activity_name] = []\n",
    "        \n",
    "        # slide_key_data = sliding_window(key_data, 128, 12)\n",
    "        # activity_data_dict[activity_name].append(slide_key_data)\n",
    "        key_data = scaler.fit_transform(key_data)\n",
    "        activity_data_dict[activity_name].append(key_data)\n",
    "        \n",
    "with h5py.File(uschad_activity_path, 'w') as f_w:\n",
    "    data_grp = f_w.create_group(name='datas')\n",
    "    \n",
    "    for key in activity_data_dict:\n",
    "        aggregated_data = np.concatenate(activity_data_dict[key])\n",
    "        \n",
    "        data_grp.create_dataset(name=key, data=aggregated_data)\n",
    "        \n",
    "        print('check shape', key, aggregated_data.shape[0])\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check shape sleeping (375000, 6)\n",
      "check shape elevator up (165491, 6)\n",
      "check shape elevator down (164999, 6)\n",
      "check shape walkingforward (381400, 6)\n",
      "check shape walkingleft (258800, 6)\n",
      "check shape walkingright (275500, 6)\n",
      "check shape walkingupstairs (211800, 6)\n",
      "check shape walkingdownstairs (197400, 6)\n",
      "check shape runningforward (176500, 6)\n",
      "check shape jumping (107100, 6)\n",
      "check shape sitting (261500, 6)\n",
      "check shape standing (236000, 6)\n"
     ]
    }
   ],
   "source": [
    "for key in activity_data_dict:\n",
    "    aggregated_data = np.concatenate(activity_data_dict[key])\n",
    "    print('check shape', key, aggregated_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping_ep49.pt\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "\n",
    "\n",
    "def find_recent_pth(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out .pt files\n",
    "    pt_files = [f for f in files if f.endswith('.pt')]\n",
    "    \n",
    "    # If no .pt files found, return None\n",
    "    if not pt_files:\n",
    "        return None\n",
    "\n",
    "    # Extract the episode number from the filename using regular expressions\n",
    "    def extract_episode_number(filename):\n",
    "        match = re.search(r'ep(\\d+)', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return -1\n",
    "    \n",
    "    # Sort the files by the extracted episode number\n",
    "    pt_files.sort(key=extract_episode_number, reverse=True)\n",
    "\n",
    "    # Return the latest .pt file (the first one after sorting)\n",
    "    return pt_files[0]\n",
    "\n",
    "folder_path = './Experiments/jumping/'\n",
    "file_path = find_recent_pth(folder_path)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_name_dict = {\n",
    "    1: \"walkingforward\",\n",
    "    2: \"walkingleft\",\n",
    "    3: \"walkingright\",\n",
    "    4: \"walkingupstairs\",\n",
    "    5: \"walkingdownstairs\",\n",
    "    6: \"runningforward\",\n",
    "    7: \"jumping\",\n",
    "    8: \"sitting\",\n",
    "    9: \"standing\",\n",
    "    10: \"sleeping\",\n",
    "    11: \"elevatorup\",\n",
    "    12: \"elevatordown\"\n",
    "}\n",
    "\n",
    "activity_label = {\n",
    "    'walkingforward': 1,\n",
    "    'walkingleft': 2,\n",
    "    'walkingright': 3,\n",
    "    'walkingupstairs': 4,\n",
    "    'walkingdownstairs': 5,\n",
    "    'runningforward': 6,\n",
    "    'jumping': 7,\n",
    "    'sitting': 8,\n",
    "    'standing': 9,\n",
    "    'sleeping': 10,\n",
    "    'elevatorup': 11,\n",
    "    'elevatordown': 12\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "output_path = '../Datas/genbylabel_dataset_400.h5'\n",
    "\n",
    "window = 400  # Size of each split window\n",
    "step = 200  # Step size for sliding window\n",
    "\n",
    "def normalize_to_neg_one_to_one(x):\n",
    "    return x * 2 - 1\n",
    "\n",
    "# Create a new HDF5 file to store splitted datasets\n",
    "with h5py.File(dataset_path, 'r') as f_r, h5py.File(output_path, 'w') as f_w:\n",
    "    datagrp = f_r['datas']\n",
    "    label_all = []\n",
    "    data_all = []\n",
    "    \n",
    "    for activity_name, dataset in datagrp.items():\n",
    "        data = dataset[:]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_fitted = scaler.fit_transform(data)\n",
    "        data_fitted = normalize_to_neg_one_to_one(data_fitted)\n",
    "        \n",
    "        num_splits = max((data.shape[0] - window + 1) // window, 0)\n",
    "        \n",
    "        for i in range(num_splits):\n",
    "            start = i * window\n",
    "            end = (i + 1) * window\n",
    "            data_all.append(data_fitted[start:end, :])\n",
    "            label_all.append(activity_label[activity_name]-1)\n",
    "        \n",
    "    f_w.create_dataset(name='datas', data=np.array(data_all))\n",
    "    f_w.create_dataset(name='labels', data=np.array(label_all, dtype='int'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "output_path = '../Datas/genbylabel_dataset_200.h5'\n",
    "\n",
    "# window = 100  # Size of each split window\n",
    "\n",
    "# Create a new HDF5 file to store splitted datasets\n",
    "with h5py.File(output_path, 'r') as f_r, h5py.File(dataset_path, 'a') as f_w:\n",
    "\n",
    "    data_all = f_r['datas'][:]\n",
    "    label_all = f_r['labels'][:]\n",
    "    \n",
    "    if 'data_genbylabel_200' in f_w:\n",
    "        del f_w['data_genbylabel_200']\n",
    "    if 'label_genbylabel_200' in f_w:\n",
    "        del f_w['label_genbylabel_200']\n",
    "    \n",
    "    f_w.create_dataset(name='data_genbylabel_200', data=data_all)\n",
    "    f_w.create_dataset(name='label_genbylabel_200', data=label_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107100, 6) 1070 1070\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "with h5py.File(dataset_path, 'r') as f_r:\n",
    "    datagrp = f_r['datas']\n",
    "    jumping  = datagrp['jumping']\n",
    "    length = (jumping.shape[0] - 100+1) // 100\n",
    "    \n",
    "    label_genbylabel = f_r['label_genbylabel'][:]\n",
    "    jumping_count = np.sum(label_genbylabel == 6)\n",
    "    \n",
    "    print(jumping.shape, length, jumping_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check activity elevatordown num_splits 822\n",
      "check activity elevatorup num_splits 825\n",
      "check activity jumping num_splits 533\n",
      "check activity runningforward num_splits 880\n",
      "check activity sitting num_splits 1305\n",
      "check activity sleeping num_splits 1873\n",
      "check activity standing num_splits 1178\n",
      "check activity walkingdownstairs num_splits 985\n",
      "check activity walkingforward num_splits 1905\n",
      "check activity walkingleft num_splits 1292\n",
      "check activity walkingright num_splits 1375\n",
      "check activity walkingupstairs num_splits 1057\n"
     ]
    }
   ],
   "source": [
    "activity_label = {\n",
    "    'walkingforward': 1,\n",
    "    'walkingleft': 2,\n",
    "    'walkingright': 3,\n",
    "    'walkingupstairs': 4,\n",
    "    'walkingdownstairs': 5,\n",
    "    'runningforward': 6,\n",
    "    'jumping': 7,\n",
    "    'sitting': 8,\n",
    "    'standing': 9,\n",
    "    'sleeping': 10,\n",
    "    'elevatorup': 11,\n",
    "    'elevatordown': 12\n",
    "}\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "window = 600  # Size of each split window\n",
    "step = 200  # Step size for sliding window\n",
    "\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "output_path = f\"../Datas/genbylabel_dataset_{window}_{step}.h5\"\n",
    "\n",
    "def normalize_to_neg_one_to_one(x):\n",
    "    return x * 2 - 1\n",
    "\n",
    "# Create a new HDF5 file to store splitted datasets\n",
    "with h5py.File(dataset_path, 'r') as f_r, h5py.File(output_path, 'w') as f_w:\n",
    "    datagrp = f_r['datas']\n",
    "    label_all = []\n",
    "    data_all = []\n",
    "    \n",
    "    for activity_name, dataset in datagrp.items():\n",
    "        data = dataset[:]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_fitted = scaler.fit_transform(data)\n",
    "        data_fitted = normalize_to_neg_one_to_one(data_fitted)\n",
    "        \n",
    "        num_splits = max((data.shape[0] - window) // step + 1, 0)\n",
    "\n",
    "        print(f\"check activity {activity_name} num_splits {num_splits}\")\n",
    "        \n",
    "        # put all activity's data into a list and all the labels, before saving to the new file\n",
    "        for i in range(num_splits):\n",
    "            start = i * step\n",
    "            end = start + window\n",
    "            data_all.append(data_fitted[start:end, :])\n",
    "            label_all.append(activity_label[activity_name] - 1)\n",
    "\n",
    "    f_w.create_dataset(name='datas', data=np.array(data_all))\n",
    "    f_w.create_dataset(name='labels', data=np.array(label_all, dtype='int'))\n",
    "    \n",
    "\n",
    "with h5py.File(output_path, 'r') as f_r, h5py.File(dataset_path, 'a') as f_w:\n",
    "\n",
    "    data_all = f_r['datas'][:]\n",
    "    label_all = f_r['labels'][:]\n",
    "    \n",
    "    if f\"data_genbylabel_{window}\" in f_w:\n",
    "        del f_w[f\"data_genbylabel_{window}\"]\n",
    "    if f\"label_genbylabel_{window}\" in f_w:\n",
    "        del f_w[f\"label_genbylabel_{window}\"]\n",
    "    \n",
    "    f_w.create_dataset(name=f\"data_genbylabel_{window}\", data=data_all)\n",
    "    f_w.create_dataset(name=f\"label_genbylabel_{window}\", data=label_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14043, 200, 6) (14043,)\n",
      "824\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as f_r:\n",
    "    # print(f_r.keys())\n",
    "    \n",
    "    data_win_200 = f_r['data_genbylabel_200'][:]\n",
    "    label_win_200 = f_r['label_genbylabel_200'][:]\n",
    "    print(data_win_200.shape, label_win_200.shape)\n",
    "    activity_count = np.sum(label_win_200 == 11)\n",
    "    print(activity_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(dataset_path, 'a') as f_r:\n",
    "    # Check if the keys exist\n",
    "    if 'data_genbylabel' in f_r and 'label_genbylabel' in f_r:\n",
    "        # Read the data from the existing keys\n",
    "        data_win_100 = f_r['data_genbylabel'][:]\n",
    "        label_win_100 = f_r['label_genbylabel'][:]\n",
    "\n",
    "        # Create new keys with updated names\n",
    "        f_r.create_dataset('data_genbylabel_100', data=data_win_100)\n",
    "        f_r.create_dataset('label_genbylabel_100', data=label_win_100)\n",
    "\n",
    "        # Delete the old keys\n",
    "        del f_r['data_genbylabel']\n",
    "        del f_r['label_genbylabel']\n",
    "\n",
    "        print(\"Keys renamed successfully.\")\n",
    "    else:\n",
    "        print(\"Original keys not found in the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14006, 1000, 6)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "dataset_path = '../Datas/uschad_activity_dataset.h5'\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as f_r:\n",
    "    data_win_1000 = f_r['data_genbylabel_1000'][:]\n",
    "    print(data_win_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maodong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
